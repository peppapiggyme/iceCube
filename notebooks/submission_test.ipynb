{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Imports\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os, random, copy, yaml, pickle\n",
    "from time import time, sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import random_split, IterableDataset\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import homophily\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import EdgeConv, knn_graph\n",
    "from torch_scatter import \\\n",
    "    scatter_add, scatter_mean, scatter_max, scatter_min\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Helper function\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import logging\n",
    "import logging.config\n",
    "import os, re, gc, psutil\n",
    "\n",
    "def get_logger(name, msg):\n",
    "    \"\"\"\n",
    "    :param name: string\n",
    "    :param msg: DEBUG, INFO, WARNING, ERROR\n",
    "    :return: Logger() instance\n",
    "    \"\"\"\n",
    "    level = {\"DEBUG\": logging.DEBUG,\n",
    "             \"INFO\": logging.INFO,\n",
    "             \"WARNING\": logging.WARNING,\n",
    "             \"ERROR\": logging.ERROR}\n",
    "    logging.basicConfig(level=level[msg],\n",
    "                        format=\"== %(name)s == %(asctime)s %(levelname)s:\\t%(message)s\",\n",
    "                        datefmt=\"%H:%M:%S\")\n",
    "    logger = logging.getLogger(name)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def walk_dir(dirname, batch_ids):\n",
    "    files = dict()\n",
    "    pattern = r\"_(\\d+)\\.parquet\"\n",
    "\n",
    "    if batch_ids is None:\n",
    "        batch_ids = list()\n",
    "        for base, _, names in os.walk(dirname):\n",
    "            for name in names:\n",
    "                match = re.findall(pattern, name)\n",
    "                batch_ids.append(int(match[0]))\n",
    "                files[int(match[0])] = os.path.join(base, name)\n",
    "        return files, batch_ids\n",
    "    \n",
    "    for base, _, names in os.walk(dirname):\n",
    "        selected_files = dict()\n",
    "        for name in names:\n",
    "            match = re.findall(pattern, name)\n",
    "            if int(match[0]) in batch_ids:\n",
    "                selected_files[int(match[0])] = os.path.join(base, name)\n",
    "        files.update(selected_files)\n",
    "    return files, batch_ids\n",
    "\n",
    "\n",
    "def memory_check(logger, msg=\"\"):\n",
    "    gc.collect()\n",
    "    logger.debug(f\"memory usage {psutil.virtual_memory().used / 1024**3:.2f} \"\n",
    "                f\"of {psutil.virtual_memory().total / 1024**3:.2f} GB {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Basic settings\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# basic\n",
    "BATCH_SIZE = 200\n",
    "BATCHES_TEST = list(range(11, 16)) #####################\n",
    "EVENTS_PER_FILE = 200_000\n",
    "\n",
    "# paths\n",
    "BASE_PATH = \"/root/autodl-tmp/kaggle/\" ######################\n",
    "PATH = os.path.join(BASE_PATH, \"icecube-neutrinos-in-deep-ice\")\n",
    "MODEL_PATH = os.path.join(BASE_PATH, \"input\", \"ice-cube-model\")\n",
    "OUTPUT_PATH = os.path.join(BASE_PATH, \"working\")\n",
    "TEST_PATH = os.path.join(PATH, \"test\")\n",
    "META_PATH = os.path.join(OUTPUT_PATH, \"test_meta\")\n",
    "\n",
    "# files\n",
    "FILES_TEST, BATCHES_TEST = walk_dir(TEST_PATH, BATCHES_TEST)\n",
    "FILE_META = os.path.join(PATH, \"test_meta.parquet\")\n",
    "FILE_SENSOR_GEO = os.path.join(PATH, \"sensor_geometry.csv\")\n",
    "FILE_GNN = os.path.join(MODEL_PATH, \"finetuned.ckpt\")\n",
    "FILE_BDT = os.path.join(MODEL_PATH, \"BDT_clf.sklearn\")\n",
    "FILE_PARAM = os.path.join(MODEL_PATH, \"parameters_local.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Split meta file\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# if not os.path.exists(META_PATH): \n",
    "#     os.mkdir(META_PATH)\n",
    "\n",
    "# meta_test = pd.read_parquet(FILE_META)\n",
    "\n",
    "# for i, df in meta_test.groupby(\"batch_id\"):\n",
    "#     print(f\"processing {i} -> {df.shape}\")\n",
    "#     splitted = os.path.join(META_PATH, f\"meta_{i}.parquet\")\n",
    "#     df.to_parquet(splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Some Logging\n",
    "# -----------------------------------------------------------------------------\n",
    "LOGGER = get_logger(\"IceCube\", \"DEBUG\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOGGER.info(f\"using {DEVICE}\")\n",
    "LOGGER.info(f\"{len(FILES_TEST)} files for testing\")\n",
    "memory_check(LOGGER)\n",
    "\n",
    "LOGGER.info(f\"GNN model:{FILE_GNN}\")\n",
    "LOGGER.info(f\"BDT model:{FILE_BDT}\")\n",
    "\n",
    "# BEST_FIT\n",
    "BEST_FIT_VALUES = None\n",
    "with open(FILE_PARAM, \"r\") as f:\n",
    "    BEST_FIT_VALUES = yaml.full_load(f)\n",
    "LOGGER.info(f\"best fit values:{BEST_FIT_VALUES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# sensor geometry\n",
    "def prepare_sensors(scale=None):\n",
    "    sensors = pd.read_csv(FILE_SENSOR_GEO).astype({\n",
    "        \"sensor_id\": np.int16,\n",
    "        \"x\": np.float32,\n",
    "        \"y\": np.float32,\n",
    "        \"z\": np.float32,\n",
    "    })\n",
    "\n",
    "    if scale is not None and isinstance(scale, float):\n",
    "        sensors[\"x\"] *= scale\n",
    "        sensors[\"y\"] *= scale\n",
    "        sensors[\"z\"] *= scale\n",
    "\n",
    "    return sensors\n",
    "\n",
    "\n",
    "def angle_to_xyz(angles_b):\n",
    "    az, zen = angles_b.t()\n",
    "    x = torch.cos(az) * torch.sin(zen)\n",
    "    y = torch.sin(az) * torch.sin(zen)\n",
    "    z = torch.cos(zen)\n",
    "    return torch.stack([x, y, z], dim=1)\n",
    "\n",
    "\n",
    "def xyz_to_angle(xyz_b):\n",
    "    x, y, z = xyz_b.t()\n",
    "    az = torch.arccos(x / torch.sqrt(x**2 + y**2)) * torch.sign(y)\n",
    "    zen = torch.arccos(z / torch.sqrt(x**2 + y**2 + z**2))\n",
    "    return torch.stack([az, zen], dim=1)\n",
    "\n",
    "\n",
    "def angular_error(xyz_pred_b, xyz_true_b):\n",
    "    return torch.arccos(torch.clip_(torch.sum(xyz_pred_b * xyz_true_b, dim=1), -1, 1))\n",
    "\n",
    "\n",
    "def angles2vector(df):\n",
    "    df[\"nx\"] = np.sin(df.zenith) * np.cos(df.azimuth)\n",
    "    df[\"ny\"] = np.sin(df.zenith) * np.sin(df.azimuth)\n",
    "    df[\"nz\"] = np.cos(df.zenith) \n",
    "    return df\n",
    "\n",
    "\n",
    "def vector2angles(n, eps=1e-8):\n",
    "    n = n / (np.linalg.norm(n, axis=1, keepdims=True) + eps)    \n",
    "    azimuth = np.arctan2( n[:,1],  n[:,0])    \n",
    "    azimuth[azimuth < 0] += 2*np.pi\n",
    "    zenith = np.arccos( n[:,2].clip(-1,1) )                                \n",
    "    return np.concatenate([azimuth[:, np.newaxis], zenith[:, np.newaxis]], axis=1)\n",
    "\n",
    "\n",
    "def series2tensor(series, set_device=None):\n",
    "    ret = torch.from_numpy(series.values).float()\n",
    "    if set_device is not None:\n",
    "        return ret.to(DEVICE)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def solve_linear(xw, yw, zw, xxw, yyw, zzw, xyw, yzw, zxw):\n",
    "    A = torch.tensor([\n",
    "        [xxw, xyw, xw],\n",
    "        [xyw, yyw, yw],\n",
    "        [xw,  yw,  1 ],\n",
    "    ])\n",
    "    b = torch.tensor([\n",
    "        zxw, yzw, zw\n",
    "    ])\n",
    "    try:\n",
    "        coeff = torch.linalg.solve(A, b)\n",
    "        return coeff\n",
    "    except Exception:\n",
    "        LOGGER.debug(\"linear system not solvable\")\n",
    "        return torch.zeros((3, ))\n",
    "\n",
    "\n",
    "def plane_fit(df, k=0, kt=0, kq=0, fun=None, eps=1e-8):\n",
    "    z_avg = series2tensor(df.z_avg)\n",
    "    t = series2tensor(df.time)\n",
    "    c = series2tensor(df.charge)\n",
    "    x = series2tensor(df.x)\n",
    "    y = series2tensor(df.y)\n",
    "    z = series2tensor(df.z)\n",
    "\n",
    "    # weighted by ...\n",
    "    w = torch.exp(-k * torch.square(z - z_avg)) \\\n",
    "        * torch.exp(-kt * t) \\\n",
    "        * torch.pow(c, kq)\n",
    "\n",
    "    # weighted values\n",
    "    xw = (x*w); xxw = (x*x*w); xyw = (x*y*w)\n",
    "    yw = (y*w); yyw = (y*y*w); yzw = (y*z*w)\n",
    "    zw = (z*w); zzw = (z*z*w); zxw = (z*x*w)  \n",
    "\n",
    "    xw = torch.sum(xw); xxw = torch.sum(xxw); xyw = torch.sum(xyw) \n",
    "    yw = torch.sum(yw); yyw = torch.sum(yyw); yzw = torch.sum(yzw) \n",
    "    zw = torch.sum(zw); zzw = torch.sum(zzw); zxw = torch.sum(zxw) \n",
    "    sumw = torch.sum(w); sumc = torch.sum(w*c); dt = torch.median(t)\n",
    "\n",
    "    sumw += eps\n",
    "    xw /= sumw; xxw /= sumw; xyw /= sumw\n",
    "    yw /= sumw; yyw /= sumw; yzw /= sumw\n",
    "    zw /= sumw; zzw /= sumw; zxw /= sumw\n",
    "\n",
    "    coeff = solve_linear(xw, yw, zw, xxw, yyw, zzw, xyw, yzw, zxw)\n",
    "    error = torch.sum((z - coeff[0] * x - coeff[1] * y - coeff[2]))\n",
    "    error *= 1e3\n",
    "    hits = w.shape[0]\n",
    "    unique_x = torch.unique(x).shape[0]\n",
    "    unique_y = torch.unique(y).shape[0]\n",
    "    unique_z = torch.unique(z).shape[0]\n",
    "\n",
    "    ret = torch.tensor([[coeff[0], coeff[1], -1, torch.square(error), hits, sumc, dt, unique_x, unique_y, unique_z]])\n",
    "    ret[:, :3] /= torch.sqrt(coeff[0]**2 + coeff[1]**2 + 1)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def prepare_df_for_plane(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # remove auxiliary\n",
    "    df = df[~df.auxiliary]\n",
    "\n",
    "    df.charge = df.charge.astype(np.float32)\n",
    "    df.charge = np.clip(df.charge, 0, 4)\n",
    "    t_min = np.min(df.time)\n",
    "    df.time = ((df.time - t_min) * 0.299792458e-3).astype(np.float32)\n",
    "    df.x *= 1e-3; df.y *= 1e-3; df.z *= 1e-3\n",
    "    \n",
    "    df[\"qz\"] = df.charge * df.z\n",
    "    centre = df.groupby([\"x\", \"y\"]).agg(\n",
    "        qsum = (\"charge\", np.sum),\n",
    "        qzsum = (\"qz\", np.sum),\n",
    "    )\n",
    "\n",
    "    centre[\"z_avg\"] = centre.qzsum / centre.qsum\n",
    "    df = pd.merge(df, centre[[\"z_avg\"]], on=[\"x\", \"y\"])\n",
    "\n",
    "    return df[[\"z_avg\", \"time\", \"charge\", \"x\", \"y\", \"z\"]]\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class IceCube(IterableDataset):\n",
    "    def __init__(\n",
    "        self, parquet_dir, meta_dir, chunk_ids,\n",
    "        batch_size=200, max_pulses=200\n",
    "    ):\n",
    "        self.parquet_dir = parquet_dir\n",
    "        self.meta_dir = meta_dir\n",
    "        self.chunk_ids = chunk_ids\n",
    "        self.batch_size = batch_size\n",
    "        self.max_pulses = max_pulses\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        chunk_ids = self.chunk_ids\n",
    "\n",
    "        # Sensor data\n",
    "        sensor = prepare_sensors()\n",
    "\n",
    "        # Read each chunk and meta iteratively into memory and build mini-batch\n",
    "        for c, chunk_id in enumerate(chunk_ids):\n",
    "            data = pd.read_parquet(os.path.join(self.parquet_dir, f\"batch_{chunk_id}.parquet\"))\n",
    "            meta = pd.read_parquet(os.path.join(self.meta_dir, f\"meta_{chunk_id}.parquet\"))\n",
    "\n",
    "\n",
    "            # Take all event_ids and split them into batches\n",
    "            eids = meta[\"event_id\"].tolist()\n",
    "            eids_batches = [\n",
    "                eids[i : i + self.batch_size]\n",
    "                for i in range(0, len(eids), self.batch_size)\n",
    "            ]\n",
    "\n",
    "            for batch_eids in eids_batches:\n",
    "                batch = []\n",
    "\n",
    "                # For each sample, extract features\n",
    "                for eid in batch_eids:\n",
    "                    df = data.loc[eid]\n",
    "                    df = pd.merge(df, sensor, on=\"sensor_id\")\n",
    "                    # sampling of pulses if number exceeds maximum\n",
    "                    if len(df) > self.max_pulses:\n",
    "                        df_pass = df[~df.auxiliary]\n",
    "                        df_fail = df[df.auxiliary]\n",
    "                        if len(df_pass) >= self.max_pulses:\n",
    "                            df = df_pass.sample(self.max_pulses)\n",
    "                        else:\n",
    "                            df_fail = df_fail.sample(self.max_pulses - len(df_pass))\n",
    "                            df = pd.concat([df_fail, df_pass])\n",
    "\n",
    "                    df.sort_values([\"time\"], inplace=True)\n",
    "\n",
    "                    t = series2tensor(df.time)\n",
    "                    c = series2tensor(df.charge)\n",
    "                    a = series2tensor(df.auxiliary)\n",
    "                    x = series2tensor(df.x)\n",
    "                    y = series2tensor(df.y)\n",
    "                    z = series2tensor(df.z)\n",
    "                    feat = torch.stack([x, y, z, t, c, a], dim=1)\n",
    "\n",
    "\n",
    "                    batch_data = Data(x=feat, n_pulses=len(feat), eid=torch.tensor([eid]).long())\n",
    "                    coeff = plane_fit(prepare_df_for_plane(df), **BEST_FIT_VALUES)\n",
    "                    setattr(batch_data, \"plane\", coeff)\n",
    "\n",
    "                    batch.append(batch_data)\n",
    "\n",
    "                yield Batch.from_data_list(batch)\n",
    "\n",
    "            del data\n",
    "            del meta\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# GraphNet GNN model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, feats):\n",
    "        layers = []\n",
    "        for i in range(1, len(feats)):\n",
    "            layers.append(nn.Linear(feats[i - 1], feats[i]))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, max_lr=1e-3, \n",
    "        num_warmup_step=1_000,\n",
    "        remaining_step=1_000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.conv0 = EdgeConv(MLP([17 * 2, 128, 256]), aggr=\"add\")\n",
    "        self.conv1 = EdgeConv(MLP([512, 336, 256]), aggr=\"add\")\n",
    "        self.conv2 = EdgeConv(MLP([512, 336, 256]), aggr=\"add\")\n",
    "        self.conv3 = EdgeConv(MLP([512, 336, 256]), aggr=\"add\")\n",
    "        self.post = MLP([1024 + 17, 336, 256])\n",
    "        self.readout = MLP([768, 128])\n",
    "        self.pred = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, data: Batch):\n",
    "        vert_feat = data.x\n",
    "        batch = data.batch\n",
    "\n",
    "        # x, y, z, t, c, a\n",
    "        # 0  1  2  3  4  5\n",
    "        vert_feat[:, 0] /= 500.0  # x\n",
    "        vert_feat[:, 1] /= 500.0  # y\n",
    "        vert_feat[:, 2] /= 500.0  # z\n",
    "        vert_feat[:, 3] = (vert_feat[:, 3] - 1.0e04) / 3.0e4  # time\n",
    "        vert_feat[:, 4] = torch.log10(vert_feat[:, 4]) / 3.0  # charge\n",
    "\n",
    "        edge_index = knn_graph(vert_feat[:, :3], 8, batch)\n",
    "\n",
    "        # Construct global features\n",
    "        hx = homophily(edge_index, vert_feat[:, 0], batch).reshape(-1, 1)\n",
    "        hy = homophily(edge_index, vert_feat[:, 1], batch).reshape(-1, 1)\n",
    "        hz = homophily(edge_index, vert_feat[:, 2], batch).reshape(-1, 1)\n",
    "        ht = homophily(edge_index, vert_feat[:, 3], batch).reshape(-1, 1)\n",
    "        means = scatter_mean(vert_feat, batch, dim=0)\n",
    "        n_p = torch.log10(data.n_pulses).reshape(-1, 1)\n",
    "        global_feats = torch.cat([means, hx, hy, hz, ht, n_p], dim=1)  # [B, 11]\n",
    "\n",
    "        # Distribute global_feats to each vertex\n",
    "        _, cnts = torch.unique_consecutive(batch, return_counts=True)\n",
    "        global_feats = torch.repeat_interleave(global_feats, cnts, dim=0)\n",
    "        vert_feat = torch.cat((vert_feat, global_feats), dim=1)\n",
    "\n",
    "        # Convolutions\n",
    "        feats = [vert_feat]\n",
    "        # Conv 0\n",
    "        vert_feat = self.conv0(vert_feat, edge_index)\n",
    "        feats.append(vert_feat)\n",
    "        # Conv 1\n",
    "        edge_index = knn_graph(vert_feat[:, :3], k=8, batch=batch)\n",
    "        vert_feat = self.conv1(vert_feat, edge_index)\n",
    "        feats.append(vert_feat)\n",
    "        # Conv 2\n",
    "        edge_index = knn_graph(vert_feat[:, :3], k=8, batch=batch)\n",
    "        vert_feat = self.conv2(vert_feat, edge_index)\n",
    "        feats.append(vert_feat)\n",
    "        # Conv 3\n",
    "        edge_index = knn_graph(vert_feat[:, :3], k=8, batch=batch)\n",
    "        vert_feat = self.conv3(vert_feat, edge_index)\n",
    "        feats.append(vert_feat)\n",
    "\n",
    "        # Postprocessing\n",
    "        post_inp = torch.cat(feats, dim=1)\n",
    "        post_out = self.post(post_inp)\n",
    "\n",
    "        # Readout\n",
    "        readout_inp = torch.cat(\n",
    "            [\n",
    "                scatter_min(post_out, batch, dim=0)[0],\n",
    "                scatter_max(post_out, batch, dim=0)[0],\n",
    "                scatter_mean(post_out, batch, dim=0),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        readout_out = self.readout(readout_inp)\n",
    "\n",
    "        # Predict\n",
    "        pred = self.pred(readout_out)\n",
    "        kappa = pred.norm(dim=1, p=2) + 1e-8\n",
    "        pred_x = pred[:, 0] / kappa\n",
    "        pred_y = pred[:, 1] / kappa\n",
    "        pred_z = pred[:, 2] / kappa\n",
    "        pred = torch.stack([pred_x, pred_y, pred_z, kappa], dim=1)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def train_or_valid_step(self, data, prefix):\n",
    "        pred_xyzk = self.forward(data)  # [B, 4]\n",
    "        true_xyz = data.gt.view(-1, 3)  # [B, 3]\n",
    "        loss = VonMisesFisher3DLoss()(pred_xyzk, true_xyz).mean()\n",
    "        error = angular_error(pred_xyzk[:, :3], true_xyz).mean()\n",
    "        self.log(f\"loss-{prefix}\", loss, batch_size=len(true_xyz), \n",
    "            on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log(f\"error-{prefix}\", error, batch_size=len(true_xyz), \n",
    "            on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, data, _):\n",
    "        return self.train_or_valid_step(data, \"train\")\n",
    "\n",
    "    def validation_step(self, data, _):\n",
    "        return self.train_or_valid_step(data, \"valid\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.max_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer,\n",
    "            schedulers=[\n",
    "                torch.optim.lr_scheduler.LinearLR(\n",
    "                    optimizer, 1e-2, 1, self.hparams.num_warmup_step\n",
    "                ),\n",
    "                torch.optim.lr_scheduler.LinearLR(\n",
    "                    optimizer, 1, 1e-3, self.hparams.remaining_step\n",
    "                ),\n",
    "            ],\n",
    "            milestones=[self.hparams.num_warmup_step],\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# GNN prediction\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def predict_gnn(model):\n",
    "\n",
    "    test_set = IceCube(TEST_PATH, META_PATH, BATCHES_TEST, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_set, batch_size=1)\n",
    "\n",
    "    pred = None\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(test_loader)):\n",
    "            xyzk = model(data.to(DEVICE))\n",
    "            x = np.concatenate([\n",
    "                xyzk[:, :3].cpu(), xyz_to_angle(xyzk[:, :3]).cpu(), data.plane.cpu()\n",
    "            ], axis=1)\n",
    "            pred = x if pred is None else np.concatenate([pred, x]) \n",
    "    \n",
    "    pred = pd.DataFrame(pred, \n",
    "        columns=[\"x\", \"y\", \"z\", \"azimuth\", \"zenith\",\n",
    "                 \"ex\", \"ey\", \"ez\", \"fit_error\", \"hits\", \"sumc\", \"dt\", \n",
    "                 \"unique_x\", \"unique_y\", \"unique_z\"])\n",
    "    \n",
    "    pred[\"azimuth\"] = np.remainder(pred[\"azimuth\"], 2 * np.pi)\n",
    "    pred[\"zenith\"] = np.remainder(pred[\"zenith\"], 2 * np.pi)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "model = Model.load_from_checkpoint(FILE_GNN)\n",
    "LOGGER.info(f\"loaded {FILE_GNN}\")\n",
    "\n",
    "model.eval()\n",
    "model.freeze()\n",
    "model.to(DEVICE)\n",
    "\n",
    "reco_df = predict_gnn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# GNN + Plane fit prediction\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "n_hat = reco_df[[\"x\", \"y\", \"z\"]].to_numpy()\n",
    "e = reco_df[[\"ex\", \"ey\", \"ez\"]].to_numpy()\n",
    "xe = np.sum(n_hat * e, axis=1)\n",
    "\n",
    "proj = n_hat - xe[:, np.newaxis] * e\n",
    "proj /= (np.linalg.norm(proj, axis=1, keepdims=True) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# GNN + Plane fit + BDT prediction\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# reco_df inputs\n",
    "reco = reco_df[[\"fit_error\", \"sumc\", \"hits\", \"zenith\", \"ez\", \"dt\", \"unique_x\", \"unique_z\"]].to_numpy()\n",
    "reco[:, 0] = np.log10(reco[:, 0] + 1e-8)\n",
    "reco[:, 1] = np.log10(reco[:, 1] + 1e-8)\n",
    "\n",
    "# load the model and predict\n",
    "LOGGER.info(\"Loading BDT model...\")\n",
    "clf = pickle.load(open(FILE_BDT, 'rb'))\n",
    "LOGGER.info(\"Predicting...\")\n",
    "X = np.concatenate([reco, np.abs(xe[:, np.newaxis])], axis=1)\n",
    "y_hat = clf.predict(X)[:, np.newaxis]\n",
    "y_hat[0][0] = True\n",
    "\n",
    "gnn = reco_df[[\"azimuth\", \"zenith\"]].to_numpy()\n",
    "fit = vector2angles(proj)\n",
    "LOGGER.info(f\"GNN prediction\\n{gnn}\")\n",
    "LOGGER.info(f\"Plane fit prediction\\n{fit}\")\n",
    "LOGGER.info(f\"BDT prediction\\n{(gnn * ~y_hat) + fit * y_hat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
